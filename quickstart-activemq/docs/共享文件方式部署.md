1、安装jdk、部署nfs
export JAVA_HOME=/home/test/jdk1.8.0_152
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$JAVA_HOME/bin:$PATH
2、找到如下配置项，将kahadb的存储路径也改为共享目录。
  <persistenceAdapter>  
      <kahaDB directory="/data1/activemq/data/kahadb"/>  
  </persistenceAdapter>  
3、启动服务
先启动的服务先获取文件权限，成为master


Activemq部署安装

第一步：
1.从官网下载最新安装包，地址：http://activemq.apache.org/download-archives.html，这里用到的是5.9.0版本。
2.安装jdk，这里不再介绍。
3.解压下载的安装包，

[html] view plain copy
tar –zxvf apache-activemq-5.9.0-bin.tar.gz  
解压后进入解压目录apache-activemq-5.9.0，运行./bin/activemqstart，便可单机运行。
第二步，集群部署：

再另外一台服务器，ip为：192.168.220.103上也同样执行第一步操作。
Activemq集群部署方式介绍：
Activemq支持多种集群部署和分布式方案：
1. 基于共享存储的主从配置，这里的存储分为共享磁盘和共享数据库。
2. networks-of-brokers ，详见http://activemq.apache.org/networks-of-brokers.html
3. 基于zookeeper和levelDB的分布式方案，5.9版本新增。

由于我们的业务场景还是比较简单的，主要是要满足mq的高可用性。因此，我重点了解了前两种方式。
第二种方式虽然可以满足服务的高可用性，客户端也可以使用failover的自切机制，但是各个服务都是独立存储数据，其他的broker只是互相作为彼此的远程消费者，如果其中一个broker宕机，那么这个broker上还没有消费的数据将也变得无法可用。
结合上述分析，我们还是采用的第一种分布式方案，主从mq去共享目录中争取锁，先获得锁的mq即为主，从库会每隔10秒尝试重新获得锁，我们采用nfs共享硬盘的方式，并使用了性能较高的SSD硬盘来作为存储媒介，数据库使用activemq默认的kahadb。

主库ip：192.168.220.69
从库ip：192.168.220.103

安装NFS服务

1.在主库上安装nfs服务：
centos 5 :
[html] view plain copy
yum install nfs-utils portmap  
centos 6 :
[html] view plain copy
yum install nfs-utils rpcbind  
我们的系统环境是centos6，因此需要安装nfs-utils和rpcbind两个包。

安装完成之后，配置/etc/exports文件。

vim /etc/exports;

然后输入：

[html] view plain copy
/data1/activemq/:192.168.220.103(rw,sync,no_root_squash,no_all_squash)  
这里需要注意no_root_squash,no_all_squash 这两项配置，意思是说，远程客户端使用共享文件时可以用root和所有用户权限访问，不然远程客户端默认用nfsnobody用户。这样会造成本地activemq启动用户创建的db和lock等文件，另一台activemq由于没有权限导致切换失败。

2. 启动rpcbind服务

[html] view plain copy
service rpcbind start  
3. 启动nfs服务

[html] view plain copy
service nfs start  
4.把这两个服务设置自启动

[html] view plain copy
chkconfig --level 35 nfs on  
chkconfig --level 35 rpcbind on  
5.需要注意selinux和iptables等安全设置。

6.在从库192.168.220.103上挂载共享目录：

[html] view plain copy
mount -t nfs192.168.220.69: /data1/activemq /data1/activemq  
7.设置从库重启系统时自动挂载：

[html] view plain copy
vim/etc/rc.d/rc.local  
加入：mount -t nfs 192.168.220.69: /data1/activemq /data1/activemq  
wq保存退出。  

配置主从的activemq

1.   修改主库conf下的activemq.xml配置文件

修改如下所示的brokerName和dataDirectory，其中dataDirectory为共享目录的路径。

[html] view plain copy
<broker xmlns="http://activemq.apache.org/schema/core" brokerName="broker69" dataDirectory="/data1/activemq/">  

找到如下配置项，将kahadb的存储路径也改为共享目录。



[html] view plain copy
<persistenceAdapter>  
    <kahaDB directory="/data1/activemq/data/kahadb"/>  
</persistenceAdapter>  

2.   从库也进行同样的配置，指定刚刚挂载的共享目录路径。

3.   在主从库上分别创建用户mq启动的账号，useradd web passwd web

这里需注意：

查看web用户可创建的进程数

su web

ulimit -u

新版的centOS 创建用户默认的进程数为1024，在高并发情况下会出现用户资源被占满的情况，需调大web用户可创建的进程数.

修改配置文件 /etc/security/limits.d/90-nproc.conf ，调成跟root用户一样即可。

 

上述两部完成之后，并完成的activemq主从集群的搭建。依次启动主从activemq，先启动的broker将优先获得文件锁，升级为主库。

activemq的安全配置

这里分两部分：消息队列的安全配置和管理后台的安全配置。

1.   消息队列的安全配置，对访问mq的客户端进行账号和密码验证

vim conf/activemq.xml

加入：

[html] view plain copy
<plugins>  
       <simpleAuthenticationPlugin>  
         <users>  
                <authenticationUser username="test"password="test~!123" groups="users,admins"/>  
        </users>  
       </simpleAuthenticationPlugin>  
</plugins>  

从库也进行同样配置。

2.   管理后台的安全配置。

1.保证conf/jetty.xml中如下中的authenticate值为true

[html] view plain copy
<bean id="securityConstraint" class="org.eclipse.jetty.util.security.Constraint">  
    <property name="name" value="BASIC" />  
    <property name="roles" value="user,admin" />  
    <!-- set authenticate=false to disable login -->  
    <property name="authenticate" value="true" />  
</bean>  


2.vimconf/jetty-realm.properties，修改相应账号密码，如下：

[html] view plain copy
# Defines users that can access the web (console, demo, etc.)  
# username: password [,rolename ...]  
admin: admin!123, admin  
user: user, user  


注意格式，第一列是用户名、第二列是密码、第三列是角色

从库也进行同样配置。



