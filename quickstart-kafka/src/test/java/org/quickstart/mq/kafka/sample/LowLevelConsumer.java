package org.quickstart.mq.kafka.sample;

import com.google.common.collect.Lists;
import lombok.extern.slf4j.Slf4j;
import org.apache.commons.collections4.CollectionUtils;
import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.OffsetAndMetadata;
import org.apache.kafka.clients.consumer.OffsetResetStrategy;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.errors.WakeupException;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.junit.Test;

import java.io.IOException;
import java.time.Duration;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Properties;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.stream.IntStream;

@Slf4j
public class LowLevelConsumer {

    private static final String brokerList = "172.16.49.66:9092,172.16.49.68:9092,172.16.49.72:9092";

    private static final long POLL_TIMEOUT = 100;

    @Test
    public void consumerMethod() {

        Consumer<String, String> consumer = createConsumer();

        String topic = "lengfeng.direct.test";

        // ä½¿ç”¨æ¶ˆè´¹è€…å¯¹è±¡è®¢é˜…è¿™äº›ä¸»é¢˜
        consumer.subscribe(Arrays.asList(topic), new SaveOffsetOnRebalance(consumer));

        // ä¸æ–­çš„è½®è¯¢è·å–ä¸»é¢˜ä¸­çš„æ¶ˆæ¯
        try {
            // poll() è·å–æ¶ˆæ¯åˆ—è¡¨ï¼Œå¯ä»¥ä¼ å…¥è¶…æ—¶æ—¶é—´
            // while (true) {
                // [åœ¨consumer.pollä¹‹åassignment()è¿”å›ä¸ºç©ºçš„é—®é¢˜](https://www.cnblogs.com/huxi2b/p/10773559.html)
                consumer.poll(0);//ä¸èƒ½ä½¿ç”¨consumer.poll(Duration.ofMillis(0));ï¼Œå¦åˆ™å¯èƒ½assignment()è¿”å›ä¸ºç©º

                consumer.assignment().forEach(topicPartition -> {
                    System.out.println(topicPartition);

                    OffsetAndMetadata offsetAndMetadata = consumer.committed(topicPartition);
                    System.out.printf("åŸå§‹çš„offset= %s%n", offsetAndMetadata);

                    Map<TopicPartition, Long> beginningOffsets = consumer.beginningOffsets(Collections.singleton(topicPartition));
                    Map<TopicPartition, Long> endOffsets = consumer.endOffsets(Collections.singleton(topicPartition));
                    System.out.println("beginningOffsets=" + beginningOffsets + ",endOffsets=" + endOffsets);


                });
                TimeUnit.SECONDS.sleep(3);
            // }
        } catch (WakeupException | InterruptedException e) {
            // ä¸ç”¨å¤„ç†è¿™ä¸ªå¼‚å¸¸ï¼Œå®ƒåªæ˜¯ç”¨æ¥åœæ­¢å¾ªç¯çš„
        } finally {
            consumer.close();
        }
    }

    @Test
    public void testAssign() {

        try {
            Consumer<String, String> consumer = createConsumer();

            String topic = "lengfeng.bi.test2";
            // consumer.assign(Arrays.asList(new TopicPartition(topic, 0), new TopicPartition(topic, 1)));

            TopicPartition partition = new TopicPartition(topic, 0);
            consumer.assign(Arrays.asList(partition));

            // consumerä»æŒ‡å®šçš„offsetå¤„ç†,å…¶å®æ˜¯é‡ç½®è¿™ä¸ªTopicPartitionçš„offset
            long seekOffset = 10;
            consumer.seek(partition, seekOffset);
            consumer.poll(Duration.ofMillis(100));
        }catch (Exception e){
            e.printStackTrace();
        }

        /*int count = 0;
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
            // records.forEach(record -> System.out
            //     .printf("topic: %s,partition: %f,offset: %f,key: %s,value: %s", record.topic(), record.partition(), record.offset(), record.key(),
            //         record.value()));

            if (records.isEmpty()) {
                System.out.println("dddddï¼Œcount=" + count++);
                if (count > 100 && count < 399) {
                    System.out.println("assign23 ");
                    consumer.assign(Arrays.asList(new TopicPartition(topic, 2), new TopicPartition(topic, 3)));
                } else if (count > 400) {
                    System.out.println("assign01");
                    consumer.assign(Arrays.asList(new TopicPartition(topic, 0), new TopicPartition(topic, 1)));
                }
            }

            records.forEach(System.out::println);
        }*/

    }

    @Test
    public void testConsumer() throws IOException {

        //å£°æ˜kafkaåˆ†åŒºæ•°ç›¸ç­‰çš„æ¶ˆè´¹çº¿ç¨‹æ•°ï¼Œä¸€ä¸ªåˆ†åŒºå¯¹åº”ä¸€ä¸ªæ¶ˆè´¹çº¿ç¨‹
        int consumeThreadNum = 1;
        //ç‰¹æ®ŠæŒ‡å®šæ¯ä¸ªåˆ†åŒºå¼€å§‹æ¶ˆè´¹çš„offset
        // List<Long> partitionOffsets = Lists.newArrayList(1111L, 1112L, 1113L, 1114L, 1115L, 1116L, 1117L, 1118L, 1119L);

        List<Long> partitionOffsets = null;

        ExecutorService executorService = Executors.newFixedThreadPool(consumeThreadNum);

        //å¾ªç¯éå†åˆ›å»ºæ¶ˆè´¹çº¿ç¨‹
        IntStream.range(0, consumeThreadNum)
            .forEach(partitionIndex -> executorService.submit(() -> startConsume(partitionIndex, partitionOffsets, consumeThreadNum)));

        System.in.read();
    }

    private void startConsume(int partitionIndex, List<Long> partitionOffsets, int consumeThreadNum) {
        //åˆ›å»ºkafka consumer
        Consumer<String, String> consumer = createConsumer();

        String topic = "topic01";

        try {
            //æŒ‡å®šè¯¥consumerå¯¹åº”çš„æ¶ˆè´¹åˆ†åŒº
            TopicPartition partition = new TopicPartition(topic, partitionIndex);
            consumer.assign(Lists.newArrayList(partition));

            //consumerçš„offsetå¤„ç†
            if (CollectionUtils.isNotEmpty(partitionOffsets) && partitionOffsets.size() == consumeThreadNum) {
                Long seekOffset = partitionOffsets.get(partitionIndex);
                log.info("partition:{} , offset seek from {}", partition, seekOffset);
                consumer.seek(partition, seekOffset);
            }

            //å¼€å§‹æ¶ˆè´¹æ•°æ®ä»»åŠ¡
            kafkaRecordConsume(consumer, partition);
        } catch (Exception e) {
            log.error("kafka consume error:{}", e);
        } finally {
            try {
                //consumer.commitSync();
            } finally {
                consumer.close();
            }
        }
    }

    private void kafkaRecordConsume(Consumer<String, String> consumer, TopicPartition partition) {
        while (true) {
            try {
                ConsumerRecords<String, String> records = consumer.poll(POLL_TIMEOUT);

                //ğŸŒ¿å¾ˆé‡è¦ï¼šæ—¥å¿—è®°å½•å½“å‰consumerçš„offsetï¼Œpartitionç›¸å…³ä¿¡æ¯(ä¹‹åå¦‚éœ€é‡æ–°æŒ‡å®šoffsetæ¶ˆè´¹å°±ä»è¿™é‡Œçš„æ—¥å¿—ä¸­è·å–offsetï¼Œpartitionä¿¡æ¯)
                if (records.count() > 0) {
                    //å…·ä½“çš„å¤„ç†æµç¨‹
                    records.forEach((k) -> handleKafkaInput(k));

                    String currentOffset = String.valueOf(consumer.position(partition));
                    log.info("current records size isï¼š{}, partition is: {}, offset is:{}", records.count(), consumer.assignment(), currentOffset);
                }

                //offsetæäº¤
                consumer.commitAsync();
            } catch (Exception e) {
                log.error("handlerKafkaInput error{}", e);
            }
        }
    }

    private void handleKafkaInput(ConsumerRecord<String, String> record) {
        System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
    }

    public static Consumer<String, String> createConsumer() {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "group2");
        props.put(ConsumerConfig.CLIENT_ID_CONFIG, "consumer3");
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false"); // æ˜¯å¦è‡ªåŠ¨æäº¤è¿›åº¦
        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, "1000");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, "100");
        // props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, OffsetResetStrategy.EARLIEST.name().toLowerCase(Locale.ROOT));

        // 2 æ„å»ºæ»¤å™¨é“¾
        // List<String> interceptors = new ArrayList<>();
        // interceptors.add(SimpleConsumerInterceptor.class.getName());
        // props.put(ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);

        Consumer<String, String> consumer = new KafkaConsumer<>(props);
        return consumer;
    }

}
